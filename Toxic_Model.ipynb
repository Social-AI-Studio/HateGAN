{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training and testing is: 159571 153164\n"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_csv('/home/ruicao/NLP/datasets/hate-speech/toxic-comment-detection/train.csv')\n",
    "test_df=pd.read_csv('/home/ruicao/NLP/datasets/hate-speech/toxic-comment-detection/test.csv')\n",
    "print('Length of training and testing is:',len(train_df),len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "[0 0 0 0 0 0]\n",
      "1 D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
      "[0 0 0 0 0 0]\n",
      "2 Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "[0 0 0 0 0 0]\n",
      "3 \"\n",
      "More\n",
      "I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n",
      "\n",
      "There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\n",
      "[0 0 0 0 0 0]\n",
      "4 You, sir, are my hero. Any chance you remember what page that's on?\n",
      "[0 0 0 0 0 0]\n",
      "5 \"\n",
      "\n",
      "Congratulations from me as well, use the tools well. Â Â· talk \"\n",
      "[0 0 0 0 0 0]\n",
      "6 COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
      "[1 1 1 0 1 0]\n",
      "7 Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.\n",
      "[0 0 0 0 0 0]\n",
      "8 Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169\n",
      "[0 0 0 0 0 0]\n",
      "9 alignment on this subject and which are contrary to those of DuLithgow\n",
      "[0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    print (i,train_df['comment_text'][i])\n",
    "    print (train_df[label_names].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
      "       'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "labels=pd.read_csv('/home/ruicao/NLP/datasets/hate-speech/toxic-comment-detection/test_labels.csv')\n",
    "print (labels.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 1 1 0 1 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(train_df[label_names].values[:10])\n",
    "print (type(train_df[label_names].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test=pd.concat([test_df['comment_text'],labels],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text'], dtype='object') Index(['comment_text', 'id', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n",
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (test_df.columns,total_test.columns)\n",
    "print (train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164\n",
      "63978\n",
      "{'0': 6090, '1': 367, '2': 3691, '3': 211, '4': 3427, '5': 712}\n"
     ]
    }
   ],
   "source": [
    "label=total_test[label_names].values\n",
    "print (len(label))\n",
    "count={str(i):0 for i in range(6)}\n",
    "counting=0\n",
    "for row in label:\n",
    "    if row[0]==-1:\n",
    "        #print (row)\n",
    "        continue\n",
    "    counting+=1\n",
    "    for j in range(6):\n",
    "        if row[j]>0.5:\n",
    "            count[str(j)]+=1\n",
    "print (counting)\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from preprocessing import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_to_delete  = '@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜.,?!-;*\"â€¦:â€”()%#$&_/\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "#isolate_dict = {ord(c): ' {} '.format(c) for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c): '' for c in symbols_to_delete}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    #x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x=clean_text(x)\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 159571\n",
      "52034\n",
      "The lenght of the dictionary is: 52035\n"
     ]
    }
   ],
   "source": [
    "#create dictionary and glove embeddings\n",
    "MIN_OCC=5\n",
    "DICT_PATH='/home/ruicao/NLP/textual/hate-speech-detection/toxic/dictionary/dictionary.pkl'\n",
    "from collections import defaultdict\n",
    "sents=[]\n",
    "for i in range(len(train_df)):\n",
    "    sent=train_df['comment_text'][i]\n",
    "    sents.append(sent)\n",
    "print ('Total number of sentences:',len(sents))\n",
    "count_word=defaultdict(int)\n",
    "for s in sents:\n",
    "    words=preprocess(s).split()\n",
    "    for w in words:\n",
    "        count_word[w]+=1\n",
    "        \n",
    "count=0\n",
    "idx2word=[]\n",
    "word2idx={}\n",
    "for word in count_word.keys():\n",
    "    if count_word[word]>=MIN_OCC:\n",
    "        word2idx[word]=count\n",
    "        count+=1\n",
    "        idx2word.append(word)\n",
    "word2idx['UNK']=count\n",
    "idx2word.append('UNK')\n",
    "print (word2idx['UNK'])\n",
    "print ('The lenght of the dictionary is:',len(idx2word))        \n",
    "pkl.dump([word2idx,idx2word],open(DICT_PATH,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52035, 300)\n"
     ]
    }
   ],
   "source": [
    "GLOVE_PATH='/home/ruicao/trained/embeddings/glove.6B.300d.txt'\n",
    "word2emb={}\n",
    "emb_path='/home/ruicao/NLP/textual/hate-speech-detection/toxic/dictionary/glove_embedding.npy'\n",
    "with open(GLOVE_PATH,'r') as f:\n",
    "    entries=f.readlines()\n",
    "    emb_dim=len(entries[0].split(' '))-1\n",
    "    weights=np.zeros((len(idx2word),emb_dim),dtype=np.float32)\n",
    "    for entry in entries:\n",
    "        word=entry.split(' ')[0]\n",
    "        word2emb[word]=np.array(list(map(float,entry.split(' ')[1:])))\n",
    "    for idx,word in enumerate(idx2word):\n",
    "        if word not in word2emb:\n",
    "            continue\n",
    "        weights[idx]=word2emb[word]\n",
    "    print (weights.shape)\n",
    "    np.save(emb_path,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Set(object):\n",
    "    def __init__(self,pd,mode='train'):\n",
    "        self.length=220\n",
    "        \n",
    "        self.dict_path='/home/ruicao/NLP/textual/hate-speech-detection/toxic/dictionary/dictionary.pkl'\n",
    "        \n",
    "        self.pd=pd\n",
    "        self.mode=mode\n",
    "        \n",
    "        self.init_dict_info()\n",
    "        self.entries=self.load_entries()\n",
    "        print ('Number of entries is:',len(self.entries))\n",
    "        self.tokenize()\n",
    "        self.tensorize()\n",
    "    \n",
    "    def init_dict_info(self):\n",
    "        self.word2idx=pkl.load(open(self.dict_path,'rb'))[0]\n",
    "        self.ntokens=len(self.word2idx)\n",
    "        print ('The length of the dictionary is:',self.ntokens)\n",
    "    \n",
    "    def get_tokens(self,sent):\n",
    "        words=preprocess(sent).split()\n",
    "        tokens=[]\n",
    "        for w in words:\n",
    "            if w in self.word2idx:\n",
    "                tokens.append(self.word2idx[w])\n",
    "            else:\n",
    "                tokens.append(self.word2idx['UNK'])\n",
    "        return tokens\n",
    "    \n",
    "    def pad_sent(self,tokens):\n",
    "        if len(tokens)<self.length:\n",
    "            padding=[self.ntokens]*(self.length-len(tokens))\n",
    "            tokens=padding+tokens\n",
    "        else:\n",
    "            tokens=tokens[:self.length]\n",
    "        return tokens\n",
    "    \n",
    "    def tokenize(self):\n",
    "        print ('Tokenizing tweets...')\n",
    "        for entry in self.entries:\n",
    "            tokens=self.get_tokens(entry['sent'])\n",
    "            #print(tokens)\n",
    "            pad_tokens=self.pad_sent(tokens)\n",
    "            entry['tokens']=np.array((pad_tokens),dtype=np.int64)\n",
    "    \n",
    "    def tensorize(self):\n",
    "        print ('Tensorizing all information...')\n",
    "        for entry in self.entries:\n",
    "            entry['tokens']=torch.from_numpy(entry['tokens'])\n",
    "            entry['label']=torch.from_numpy(entry['label'])\n",
    "    \n",
    "    def load_entries(self):\n",
    "        entries=[]\n",
    "        texts=self.pd['comment_text']\n",
    "        labels=self.pd[label_names].values\n",
    "        for i,sent in enumerate(texts):\n",
    "            if self.mode=='test' and labels[i][0]==-1:\n",
    "                continue\n",
    "            entry={\n",
    "                'sent':sent,\n",
    "                'label':labels[i]\n",
    "            }\n",
    "            entries.append(entry)\n",
    "        return entries    \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        entry=self.entries[index]\n",
    "        tokens=entry['tokens']\n",
    "        label=entry['label']\n",
    "        return tokens,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dictionary is: 52035\n",
      "Number of entries is: 159571\n",
      "Tokenizing tweets...\n",
      "Tensorizing all information...\n"
     ]
    }
   ],
   "source": [
    "train_set=Data_Set(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dictionary is: 52035\n",
      "Number of entries is: 63978\n",
      "Tokenizing tweets...\n",
      "Tensorizing all information...\n"
     ]
    }
   ],
   "source": [
    "test_set=Data_Set(total_test,mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Base_Model(nn.Module):\n",
    "    #vocab_size is ntokens + 1\n",
    "    def __init__(self,emb_dim,vocab_size,num_hidden,final_dim,emb_dropout,fc_dropout,emb_path):\n",
    "        super(Base_Model,self).__init__()\n",
    "        self.emb=nn.Embedding(vocab_size+1,emb_dim,padding_idx=vocab_size)\n",
    "        self.lstm1=nn.LSTM(emb_dim,num_hidden,bidirectional=True,batch_first=True)\n",
    "        self.lstm2=nn.LSTM(num_hidden*2,num_hidden,bidirectional=True,batch_first=True)\n",
    "        self.out=nn.Linear(num_hidden*4,6)\n",
    "        \n",
    "        self.emb_drop=nn.Dropout(emb_dropout)\n",
    "        self.init_word_emb(emb_path)\n",
    "        \n",
    "    def init_word_emb(self,path):\n",
    "        print ('Initializing the pre-trained embeddings')\n",
    "        emb_weights=torch.from_numpy(np.load(path))\n",
    "        self.emb.weight.data[:-1]=emb_weights\n",
    "        \n",
    "    def forward(self,tokens):\n",
    "        t_emb=self.emb(tokens)\n",
    "        t_emb=self.emb_drop(t_emb)\n",
    "        hidden1,_=self.lstm1(t_emb)\n",
    "        hidden2,_=self.lstm2(hidden1)\n",
    "        avg_pool=torch.mean(hidden2,1)\n",
    "        max_pool,_=torch.max(hidden2,1)\n",
    "        concat_hidden=torch.cat((max_pool,avg_pool),1)\n",
    "        result=torch.sigmoid(self.out(concat_hidden)) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM=300\n",
    "VOCAB_SIZE=train_set.ntokens\n",
    "NUM_HIDDEN=128\n",
    "FINAL_DIM=6\n",
    "EMB_DROPOUT=0.3\n",
    "FC_DROPOUT=0.5\n",
    "EMB_PATH='/home/ruicao/NLP/textual/hate-speech-detection/toxic/dictionary/glove_embedding.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "CUDA_DEVICE=1\n",
    "SEED=1111\n",
    "torch.cuda.set_device(CUDA_DEVICE)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda:\"+str(CUDA_DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "model=Base_Model(EMB_DIM,VOCAB_SIZE,NUM_HIDDEN,FINAL_DIM,EMB_DROPOUT,FC_DROPOUT,EMB_PATH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256\n",
    "EPOCHS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_loader=DataLoader(test_set,BATCH_SIZE,shuffle=True,num_workers=1)\n",
    "train_loader=DataLoader(train_set,BATCH_SIZE,shuffle=True,num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multi_loss(r_pred,r_labels):\n",
    "    loss=criterion(r_pred,r_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(r_pred,r_labels):\n",
    "    r_pred=r_pred.detach().cpu().numpy()\n",
    "    r_labels=r_labels.detach().cpu().numpy()\n",
    "    result=roc_auc_score(r_labels,r_pred,average='weighted')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(baseline,test_info):\n",
    "    t_loss=0.0\n",
    "    print ('Length of iterations for evaluation is:',len(test_info))\n",
    "    for i,(tokens,labels) in enumerate(test_info):\n",
    "        with torch.no_grad():\n",
    "            tokens=tokens.to(device)\n",
    "            labels=labels.float().to(device)\n",
    "            #print (labels)\n",
    "            pred=baseline(tokens)\n",
    "            b_loss=compute_multi_loss(pred,labels)\n",
    "            t_loss+=b_loss\n",
    "        if i==0:\n",
    "            t_pred=pred\n",
    "            t_labels=labels\n",
    "        else:\n",
    "            t_pred=torch.cat((t_pred,pred),dim=0)\n",
    "            t_labels=torch.cat((t_labels,labels),dim=0)\n",
    "    avg_loss=t_loss\n",
    "    avg_score=compute_score(t_pred,t_labels)\n",
    "    return avg_loss,avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim=torch.optim.Adamax(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and score is tensor(53.2302, device='cuda:1', grad_fn=<AddBackward0>) 0.9210070221431018 in Epoch 0\n",
      "Length of iterations for evaluation is: 250\n",
      "Evaluation loss and score is tensor(20.8438, device='cuda:1') 0.9475581242127151 in Epoch 0\n",
      "Training loss and score is tensor(34.8857, device='cuda:1', grad_fn=<AddBackward0>) 0.971088738889286 in Epoch 1\n",
      "Length of iterations for evaluation is: 250\n",
      "Evaluation loss and score is tensor(20.1914, device='cuda:1') 0.9571523526913058 in Epoch 1\n",
      "Training loss and score is tensor(31.8086, device='cuda:1', grad_fn=<AddBackward0>) 0.9772264089984338 in Epoch 2\n",
      "Length of iterations for evaluation is: 250\n",
      "Evaluation loss and score is tensor(19.4898, device='cuda:1') 0.9607476777147577 in Epoch 2\n",
      "Training loss and score is tensor(29.6564, device='cuda:1', grad_fn=<AddBackward0>) 0.9813206903534551 in Epoch 3\n",
      "Length of iterations for evaluation is: 250\n",
      "Evaluation loss and score is tensor(18.3606, device='cuda:1') 0.9657028118996226 in Epoch 3\n",
      "Training loss and score is tensor(27.7792, device='cuda:1', grad_fn=<AddBackward0>) 0.9844246964405461 in Epoch 4\n",
      "Length of iterations for evaluation is: 250\n",
      "Evaluation loss and score is tensor(17.3896, device='cuda:1') 0.9675574221133868 in Epoch 4\n",
      "Training loss and score is tensor(26.2073, device='cuda:1', grad_fn=<AddBackward0>) 0.9864626271627298 in Epoch 5\n",
      "Length of iterations for evaluation is: 250\n",
      "Evaluation loss and score is tensor(16.6415, device='cuda:1') 0.9693697361587172 in Epoch 5\n",
      "Training loss and score is tensor(24.7913, device='cuda:1', grad_fn=<AddBackward0>) 0.9882986947355293 in Epoch 6\n",
      "Length of iterations for evaluation is: 250\n",
      "Evaluation loss and score is tensor(18.3227, device='cuda:1') 0.9700237364954787 in Epoch 6\n",
      "Training loss and score is tensor(23.3442, device='cuda:1', grad_fn=<AddBackward0>) 0.989951874216018 in Epoch 7\n",
      "Length of iterations for evaluation is: 250\n",
      "Evaluation loss and score is tensor(18.8487, device='cuda:1') 0.9685633197725426 in Epoch 7\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    total_loss=0.0\n",
    "    model.train(True)\n",
    "    for i,(tokens,labels) in enumerate(train_loader):\n",
    "        tokens=tokens.to(device)\n",
    "        labels=labels.float().to(device)\n",
    "        pred=model(tokens)\n",
    "        #print (type(labels))\n",
    "        loss=compute_multi_loss(pred,labels)\n",
    "        total_loss+=loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        if i==0:\n",
    "            t_pred=pred\n",
    "            t_labels=labels\n",
    "        else:\n",
    "            t_pred=torch.cat((t_pred,pred),dim=0)\n",
    "            t_labels=torch.cat((t_labels,labels),dim=0)\n",
    "    train_score=compute_score(t_pred,t_labels)\n",
    "    print ('Training loss and score is',total_loss,train_score,'in Epoch',epoch)\n",
    "    model.train(False)\n",
    "    eval_loss,eval_score=evaluate_model(model,test_loader)\n",
    "    print ('Evaluation loss and score is',eval_loss,eval_score,'in Epoch',epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/ruicao/NLP/textual/hate-speech-detection/toxic/model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
